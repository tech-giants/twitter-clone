{"ast":null,"code":"/*\n\tMIT License http://www.opensource.org/licenses/mit-license.php\n\tAuthor Tobias Koppers @sokra\n*/\n\"use strict\";\n\nclass MergeDuplicateChunksPlugin {\n  apply(compiler) {\n    compiler.hooks.compilation.tap(\"MergeDuplicateChunksPlugin\", compilation => {\n      compilation.hooks.optimizeChunksBasic.tap(\"MergeDuplicateChunksPlugin\", chunks => {\n        // remember already tested chunks for performance\n        const notDuplicates = new Set(); // for each chunk\n\n        for (const chunk of chunks) {\n          // track a Set of all chunk that could be duplicates\n          let possibleDuplicates;\n\n          for (const module of chunk.modulesIterable) {\n            if (possibleDuplicates === undefined) {\n              // when possibleDuplicates is not yet set,\n              // create a new Set from chunks of the current module\n              // including only chunks with the same number of modules\n              for (const dup of module.chunksIterable) {\n                if (dup !== chunk && chunk.getNumberOfModules() === dup.getNumberOfModules() && !notDuplicates.has(dup)) {\n                  // delay allocating the new Set until here, reduce memory pressure\n                  if (possibleDuplicates === undefined) {\n                    possibleDuplicates = new Set();\n                  }\n\n                  possibleDuplicates.add(dup);\n                }\n              } // when no chunk is possible we can break here\n\n\n              if (possibleDuplicates === undefined) break;\n            } else {\n              // validate existing possible duplicates\n              for (const dup of possibleDuplicates) {\n                // remove possible duplicate when module is not contained\n                if (!dup.containsModule(module)) {\n                  possibleDuplicates.delete(dup);\n                }\n              } // when all chunks has been removed we can break here\n\n\n              if (possibleDuplicates.size === 0) break;\n            }\n          } // when we found duplicates\n\n\n          if (possibleDuplicates !== undefined && possibleDuplicates.size > 0) {\n            for (const otherChunk of possibleDuplicates) {\n              if (otherChunk.hasRuntime() !== chunk.hasRuntime()) continue; // merge them\n\n              if (chunk.integrate(otherChunk, \"duplicate\")) {\n                chunks.splice(chunks.indexOf(otherChunk), 1);\n              }\n            }\n          } // don't check already processed chunks twice\n\n\n          notDuplicates.add(chunk);\n        }\n      });\n    });\n  }\n\n}\n\nmodule.exports = MergeDuplicateChunksPlugin;","map":{"version":3,"sources":["/Users/hamzakhan/Documents/twitter-react/twitter/node_modules/webpack/lib/optimize/MergeDuplicateChunksPlugin.js"],"names":["MergeDuplicateChunksPlugin","apply","compiler","hooks","compilation","tap","optimizeChunksBasic","chunks","notDuplicates","Set","chunk","possibleDuplicates","module","modulesIterable","undefined","dup","chunksIterable","getNumberOfModules","has","add","containsModule","delete","size","otherChunk","hasRuntime","integrate","splice","indexOf","exports"],"mappings":"AAAA;AACA;AACA;AACA;AACA;;AAEA,MAAMA,0BAAN,CAAiC;AAChCC,EAAAA,KAAK,CAACC,QAAD,EAAW;AACfA,IAAAA,QAAQ,CAACC,KAAT,CAAeC,WAAf,CAA2BC,GAA3B,CACC,4BADD,EAECD,WAAW,IAAI;AACdA,MAAAA,WAAW,CAACD,KAAZ,CAAkBG,mBAAlB,CAAsCD,GAAtC,CACC,4BADD,EAECE,MAAM,IAAI;AACT;AACA,cAAMC,aAAa,GAAG,IAAIC,GAAJ,EAAtB,CAFS,CAIT;;AACA,aAAK,MAAMC,KAAX,IAAoBH,MAApB,EAA4B;AAC3B;AACA,cAAII,kBAAJ;;AACA,eAAK,MAAMC,MAAX,IAAqBF,KAAK,CAACG,eAA3B,EAA4C;AAC3C,gBAAIF,kBAAkB,KAAKG,SAA3B,EAAsC;AACrC;AACA;AACA;AACA,mBAAK,MAAMC,GAAX,IAAkBH,MAAM,CAACI,cAAzB,EAAyC;AACxC,oBACCD,GAAG,KAAKL,KAAR,IACAA,KAAK,CAACO,kBAAN,OAA+BF,GAAG,CAACE,kBAAJ,EAD/B,IAEA,CAACT,aAAa,CAACU,GAAd,CAAkBH,GAAlB,CAHF,EAIE;AACD;AACA,sBAAIJ,kBAAkB,KAAKG,SAA3B,EAAsC;AACrCH,oBAAAA,kBAAkB,GAAG,IAAIF,GAAJ,EAArB;AACA;;AACDE,kBAAAA,kBAAkB,CAACQ,GAAnB,CAAuBJ,GAAvB;AACA;AACD,eAhBoC,CAiBrC;;;AACA,kBAAIJ,kBAAkB,KAAKG,SAA3B,EAAsC;AACtC,aAnBD,MAmBO;AACN;AACA,mBAAK,MAAMC,GAAX,IAAkBJ,kBAAlB,EAAsC;AACrC;AACA,oBAAI,CAACI,GAAG,CAACK,cAAJ,CAAmBR,MAAnB,CAAL,EAAiC;AAChCD,kBAAAA,kBAAkB,CAACU,MAAnB,CAA0BN,GAA1B;AACA;AACD,eAPK,CAQN;;;AACA,kBAAIJ,kBAAkB,CAACW,IAAnB,KAA4B,CAAhC,EAAmC;AACnC;AACD,WAlC0B,CAoC3B;;;AACA,cACCX,kBAAkB,KAAKG,SAAvB,IACAH,kBAAkB,CAACW,IAAnB,GAA0B,CAF3B,EAGE;AACD,iBAAK,MAAMC,UAAX,IAAyBZ,kBAAzB,EAA6C;AAC5C,kBAAIY,UAAU,CAACC,UAAX,OAA4Bd,KAAK,CAACc,UAAN,EAAhC,EAAoD,SADR,CAE5C;;AACA,kBAAId,KAAK,CAACe,SAAN,CAAgBF,UAAhB,EAA4B,WAA5B,CAAJ,EAA8C;AAC7ChB,gBAAAA,MAAM,CAACmB,MAAP,CAAcnB,MAAM,CAACoB,OAAP,CAAeJ,UAAf,CAAd,EAA0C,CAA1C;AACA;AACD;AACD,WAhD0B,CAkD3B;;;AACAf,UAAAA,aAAa,CAACW,GAAd,CAAkBT,KAAlB;AACA;AACD,OA5DF;AA8DA,KAjEF;AAmEA;;AArE+B;;AAuEjCE,MAAM,CAACgB,OAAP,GAAiB5B,0BAAjB","sourcesContent":["/*\n\tMIT License http://www.opensource.org/licenses/mit-license.php\n\tAuthor Tobias Koppers @sokra\n*/\n\"use strict\";\n\nclass MergeDuplicateChunksPlugin {\n\tapply(compiler) {\n\t\tcompiler.hooks.compilation.tap(\n\t\t\t\"MergeDuplicateChunksPlugin\",\n\t\t\tcompilation => {\n\t\t\t\tcompilation.hooks.optimizeChunksBasic.tap(\n\t\t\t\t\t\"MergeDuplicateChunksPlugin\",\n\t\t\t\t\tchunks => {\n\t\t\t\t\t\t// remember already tested chunks for performance\n\t\t\t\t\t\tconst notDuplicates = new Set();\n\n\t\t\t\t\t\t// for each chunk\n\t\t\t\t\t\tfor (const chunk of chunks) {\n\t\t\t\t\t\t\t// track a Set of all chunk that could be duplicates\n\t\t\t\t\t\t\tlet possibleDuplicates;\n\t\t\t\t\t\t\tfor (const module of chunk.modulesIterable) {\n\t\t\t\t\t\t\t\tif (possibleDuplicates === undefined) {\n\t\t\t\t\t\t\t\t\t// when possibleDuplicates is not yet set,\n\t\t\t\t\t\t\t\t\t// create a new Set from chunks of the current module\n\t\t\t\t\t\t\t\t\t// including only chunks with the same number of modules\n\t\t\t\t\t\t\t\t\tfor (const dup of module.chunksIterable) {\n\t\t\t\t\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t\t\t\t\tdup !== chunk &&\n\t\t\t\t\t\t\t\t\t\t\tchunk.getNumberOfModules() === dup.getNumberOfModules() &&\n\t\t\t\t\t\t\t\t\t\t\t!notDuplicates.has(dup)\n\t\t\t\t\t\t\t\t\t\t) {\n\t\t\t\t\t\t\t\t\t\t\t// delay allocating the new Set until here, reduce memory pressure\n\t\t\t\t\t\t\t\t\t\t\tif (possibleDuplicates === undefined) {\n\t\t\t\t\t\t\t\t\t\t\t\tpossibleDuplicates = new Set();\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\tpossibleDuplicates.add(dup);\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t// when no chunk is possible we can break here\n\t\t\t\t\t\t\t\t\tif (possibleDuplicates === undefined) break;\n\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\t// validate existing possible duplicates\n\t\t\t\t\t\t\t\t\tfor (const dup of possibleDuplicates) {\n\t\t\t\t\t\t\t\t\t\t// remove possible duplicate when module is not contained\n\t\t\t\t\t\t\t\t\t\tif (!dup.containsModule(module)) {\n\t\t\t\t\t\t\t\t\t\t\tpossibleDuplicates.delete(dup);\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t// when all chunks has been removed we can break here\n\t\t\t\t\t\t\t\t\tif (possibleDuplicates.size === 0) break;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// when we found duplicates\n\t\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t\tpossibleDuplicates !== undefined &&\n\t\t\t\t\t\t\t\tpossibleDuplicates.size > 0\n\t\t\t\t\t\t\t) {\n\t\t\t\t\t\t\t\tfor (const otherChunk of possibleDuplicates) {\n\t\t\t\t\t\t\t\t\tif (otherChunk.hasRuntime() !== chunk.hasRuntime()) continue;\n\t\t\t\t\t\t\t\t\t// merge them\n\t\t\t\t\t\t\t\t\tif (chunk.integrate(otherChunk, \"duplicate\")) {\n\t\t\t\t\t\t\t\t\t\tchunks.splice(chunks.indexOf(otherChunk), 1);\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// don't check already processed chunks twice\n\t\t\t\t\t\t\tnotDuplicates.add(chunk);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t);\n\t\t\t}\n\t\t);\n\t}\n}\nmodule.exports = MergeDuplicateChunksPlugin;\n"]},"metadata":{},"sourceType":"script"}